{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inten = pd.read_csv('data/depart_train.csv')\n",
    "# sns.countplot(x=\"label\", data=inten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inten = pd.read_csv('data/inten_train.csv')\n",
    "# sns.countplot(x=\"label\", data=inten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== inten_best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== depart_best_model.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0221283920109272"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import *\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torchtext.data import Field, LabelField, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        options_name = \"bert-base-chinese\"\n",
    "        #BertForSequenceClassification.config_class = 14\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name,num_labels = 14)\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "        #print(self.encoder(text, labels=label))\n",
    "        return loss, text_fea\n",
    "    \n",
    "model_inten = BERT().to(device)\n",
    "load_checkpoint('inten_best_model.pt', model_inten)\n",
    "\n",
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        options_name = \"bert-base-chinese\"\n",
    "        #BertForSequenceClassification.config_class = 14\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name,num_labels = 15)\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "        #print(self.encoder(text, labels=label))\n",
    "        return loss, text_fea\n",
    "\n",
    "model_depart = BERT().to(device)\n",
    "load_checkpoint('depart_best_model.pt', model_depart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "with open('inten_dic.json') as json_file:\n",
    "    dic = json.load(json_file)\n",
    "inten_dic = {v:k for k,v in zip(dic.keys(),dic.values())}\n",
    "\n",
    "with open('depart_dic.json') as json_file:\n",
    "    dic = json.load(json_file)\n",
    "depart_dic = {v:k for k,v in zip(dic.keys(),dic.values())}\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "torch.cuda.get_device_name(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "label_field = Field(sequential=False, tokenize=y_tokenize, use_vocab=False, batch_first=True)\n",
    "text_field  = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('label', label_field),('titletext', text_field)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA(Qu):\n",
    "    df = pd.DataFrame()\n",
    "    df['label'] = [0]\n",
    "    df['titletext'] = [Qu]\n",
    "    df.to_csv('data/inten_test.csv', index=False)\n",
    "    # TabularDataset\n",
    "\n",
    "    _,test = TabularDataset.splits(path='./', train='data/inten_valid.csv',test='data/inten_test.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "    test_iter = BucketIterator(test, batch_size=64, sort_key=lambda x: len(x.titletext),\n",
    "                                device=device, train=True, sort=True, sort_within_batch=True)\n",
    "\n",
    "    test_loader = test_iter\n",
    "    model_inten.eval()\n",
    "    model_depart.eval()\n",
    "    pred_inten,pred_depart = [],[]\n",
    "    with torch.no_grad():\n",
    "        for i in test_iter:\n",
    "                labels = i.label\n",
    "                titletext = i.titletext\n",
    "                labels = labels.type(torch.LongTensor)           \n",
    "                labels = labels.to(device)\n",
    "                titletext = titletext.type(torch.LongTensor)  \n",
    "                titletext = titletext.to(device)\n",
    "                _, output_inten = model_inten(titletext, labels)\n",
    "                _, output_depart = model_depart(titletext, labels)\n",
    "\n",
    "                pred_inten.extend(torch.argmax(output_inten, 1).tolist())\n",
    "                pred_depart.extend(torch.argmax(output_depart, 1).tolist())\n",
    "            \n",
    "    return inten_dic[pred_inten[0]],depart_dic[pred_depart[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('畢業', '企業管理學系')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA('企管系可以提前畢業嗎')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('面試', '產業經濟學系')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA('產經系面試要戴口罩嗎')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('學系介紹', '公共行政學系')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA('公行系有那些老師')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('學系介紹', '統計學系')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA('統計系跟其他科系有什麼不同')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
